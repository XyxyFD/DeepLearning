{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network, experimentation tool, version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# activation functions\n",
    "# ReLu is very simple, it filters out all negative values\n",
    "# this is a powerful activation function in reality\n",
    "def activation_ReLu(number):\n",
    "    if number > 0:\n",
    "        return number\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# we also need a derivated version of ReLu\n",
    "# otherwise same as original, but instead of original value, return 1 instead\n",
    "def activation_ReLu_partial_derivative(number):\n",
    "    if number > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lock down the randomness in order to get same results everytime\n",
    "# you can change or disable this if you want\n",
    "np.random.seed(123)\n",
    "\n",
    "def generate_train_data():\n",
    "    result = []\n",
    "\n",
    "    # create 100 numbers\n",
    "    for x in range(100):\n",
    "        n1 = np.random.randint(0, 5)\n",
    "        n2 = np.random.randint(3, 7)\n",
    "\n",
    "        # formula for the target variable: x1 ^^ 2 + x2 + (random integer between 0-5)\n",
    "        # the only point of this is to have some kind of logic in the data\n",
    "        n3 = n1 ** 2 + n2 + np.random.randint(0, 5)\n",
    "        n3 = int(n3)\n",
    "\n",
    "        result.append([n1, n2, n3])\n",
    "\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss: 28.389667713167928\n",
      "Epoch: 2, loss: 10.184948530814802\n",
      "Epoch: 3, loss: 6.8566742246444425\n",
      "Epoch: 4, loss: 5.203997555936813\n",
      "Epoch: 5, loss: 4.560412950435632\n",
      "Epoch: 6, loss: 4.353402578167234\n",
      "Epoch: 7, loss: 4.291869063103325\n",
      "Epoch: 8, loss: 4.271655968225747\n",
      "Epoch: 9, loss: 4.262421963304822\n",
      "Epoch: 10, loss: 4.256183120015454\n",
      "Epoch: 11, loss: 4.2509011267520895\n",
      "Epoch: 12, loss: 4.246040056502573\n",
      "Epoch: 13, loss: 4.2414443225447265\n",
      "Epoch: 14, loss: 4.237059450193223\n",
      "Epoch: 15, loss: 4.2328606537870295\n",
      "Epoch: 16, loss: 4.228833239220659\n",
      "Epoch: 17, loss: 4.224966584368132\n",
      "Epoch: 18, loss: 4.221252013945839\n",
      "Epoch: 19, loss: 4.217681940930515\n",
      "Epoch: 20, loss: 4.214249479478713\n",
      "Epoch: 21, loss: 4.210948254527628\n",
      "Epoch: 22, loss: 4.207772300258078\n",
      "Epoch: 23, loss: 4.204716000888227\n",
      "Epoch: 24, loss: 4.201774052460182\n",
      "Epoch: 25, loss: 4.198941435476017\n",
      "Epoch: 26, loss: 4.196213393449037\n",
      "Epoch: 27, loss: 4.19358541492409\n",
      "Epoch: 28, loss: 4.191053217726361\n",
      "Epoch: 29, loss: 4.188612734788968\n",
      "Epoch: 30, loss: 4.186260101201793\n",
      "Epoch: 31, loss: 4.1839916422704935\n",
      "Epoch: 32, loss: 4.181803862449725\n",
      "Epoch: 33, loss: 4.179693435054215\n",
      "Epoch: 34, loss: 4.17765719267347\n",
      "Epoch: 35, loss: 4.17569211822906\n",
      "Epoch: 36, loss: 4.173795336622101\n",
      "Epoch: 37, loss: 4.1719641069246\n",
      "Epoch: 38, loss: 4.170195815073221\n",
      "Epoch: 39, loss: 4.168487967027811\n",
      "Epoch: 40, loss: 4.166838182360457\n",
      "Epoch: 41, loss: 4.165244188243746\n",
      "Epoch: 42, loss: 4.1637038138095575\n",
      "Epoch: 43, loss: 4.162214984851975\n",
      "Epoch: 44, loss: 4.16077571885019\n",
      "Epoch: 45, loss: 4.159384120289067\n",
      "Epoch: 46, loss: 4.158038376256862\n",
      "Epoch: 47, loss: 4.156736752301205\n",
      "Epoch: 48, loss: 4.155477588525862\n",
      "Epoch: 49, loss: 4.154259295912198\n",
      "Epoch: 50, loss: 4.153080352850429\n",
      "Epoch: 51, loss: 4.151939301866914\n",
      "Epoch: 52, loss: 4.150834746534724\n",
      "Epoch: 53, loss: 4.149765348555684\n",
      "Epoch: 54, loss: 4.148729825002983\n",
      "Epoch: 55, loss: 4.147726945714148\n",
      "Epoch: 56, loss: 4.146755530825006\n",
      "Epoch: 57, loss: 4.145814448435854\n",
      "Epoch: 58, loss: 4.14490261240174\n",
      "Epoch: 59, loss: 4.144018980239226\n",
      "Epoch: 60, loss: 4.1431625511426775\n",
      "Epoch: 61, loss: 4.14233236410344\n",
      "Epoch: 62, loss: 4.141527496125839\n",
      "Epoch: 63, loss: 4.14074706053427\n",
      "Epoch: 64, loss: 4.139990205366111\n",
      "Epoch: 65, loss: 4.139256111845418\n",
      "Epoch: 66, loss: 4.138543992932833\n",
      "Epoch: 67, loss: 4.137853091947306\n",
      "Epoch: 68, loss: 4.1371826812556325\n",
      "Epoch: 69, loss: 4.136532061025946\n",
      "Epoch: 70, loss: 4.135900558041673\n",
      "Epoch: 71, loss: 4.13528752457257\n",
      "Epoch: 72, loss: 4.13469233729972\n",
      "Epoch: 73, loss: 4.134114396291619\n",
      "Epoch: 74, loss: 4.133553124028502\n",
      "Epoch: 75, loss: 4.133007964472429\n",
      "Epoch: 76, loss: 4.132478382180622\n",
      "Epoch: 77, loss: 4.131963861459838\n",
      "Epoch: 78, loss: 4.131463905559574\n",
      "Epoch: 79, loss: 4.130978035902156\n",
      "Epoch: 80, loss: 4.130505791347732\n",
      "Epoch: 81, loss: 4.130046727492454\n",
      "Epoch: 82, loss: 4.129600415998126\n",
      "Epoch: 83, loss: 4.129166443951727\n",
      "Epoch: 84, loss: 4.128744413253335\n",
      "Epoch: 85, loss: 4.128333940031013\n",
      "Epoch: 86, loss: 4.127934654081344\n",
      "Epoch: 87, loss: 4.12754619833435\n",
      "Epoch: 88, loss: 4.127168228341583\n",
      "Epoch: 89, loss: 4.126800411786312\n",
      "Epoch: 90, loss: 4.126442428014683\n",
      "Epoch: 91, loss: 4.126093967586903\n",
      "Epoch: 92, loss: 4.125754731847455\n",
      "Epoch: 93, loss: 4.12542443251346\n",
      "Epoch: 94, loss: 4.125102791280338\n",
      "Epoch: 95, loss: 4.124789539443956\n",
      "Epoch: 96, loss: 4.124484417538481\n",
      "Epoch: 97, loss: 4.124187174989249\n",
      "Epoch: 98, loss: 4.123897569779911\n",
      "Epoch: 99, loss: 4.123615368133257\n",
      "Epoch: 100, loss: 4.12334034420507\n",
      "Epoch: 101, loss: 4.123072279790393\n",
      "Epoch: 102, loss: 4.122810964041733\n",
      "Epoch: 103, loss: 4.12255619319857\n",
      "Epoch: 104, loss: 4.1223077703277555\n",
      "Epoch: 105, loss: 4.122065505074249\n",
      "Epoch: 106, loss: 4.1218292134217975\n",
      "Epoch: 107, loss: 4.121598717463083\n",
      "Epoch: 108, loss: 4.121373845178937\n",
      "Epoch: 109, loss: 4.1211544302262295\n",
      "Epoch: 110, loss: 4.120940311734081\n",
      "Epoch: 111, loss: 4.120731334107999\n",
      "Epoch: 112, loss: 4.120527346841621\n",
      "Epoch: 113, loss: 4.120328204335756\n",
      "Epoch: 114, loss: 4.120133765724395\n",
      "Epoch: 115, loss: 4.119943894707415\n",
      "Epoch: 116, loss: 4.11975845938965\n",
      "Epoch: 117, loss: 4.119577332126157\n",
      "Epoch: 118, loss: 4.119400389373302\n",
      "Epoch: 119, loss: 4.119227511545521\n",
      "Epoch: 120, loss: 4.119058582877483\n",
      "Epoch: 121, loss: 4.118893491291422\n",
      "Epoch: 122, loss: 4.118732128269454\n",
      "Epoch: 123, loss: 4.118574388730661\n",
      "Epoch: 124, loss: 4.118420170912758\n",
      "Epoch: 125, loss: 4.11826937625813\n",
      "Epoch: 126, loss: 4.118121909304116\n",
      "Epoch: 127, loss: 4.1179776775773265\n",
      "Epoch: 128, loss: 4.1178365914918285\n",
      "Epoch: 129, loss: 4.117698564251103\n",
      "Epoch: 130, loss: 4.1175635117535485\n",
      "Epoch: 131, loss: 4.117431352501442\n",
      "Epoch: 132, loss: 4.117302007513219\n",
      "Epoch: 133, loss: 4.1171754002389\n",
      "Epoch: 134, loss: 4.11705145647861\n",
      "Epoch: 135, loss: 4.1169301043040045\n",
      "Epoch: 136, loss: 4.116811273982524\n",
      "Epoch: 137, loss: 4.116694897904373\n",
      "Epoch: 138, loss: 4.1165809105120905\n",
      "Epoch: 139, loss: 4.116469248232637\n",
      "Epoch: 140, loss: 4.116359849411892\n",
      "Epoch: 141, loss: 4.116252654251488\n",
      "Epoch: 142, loss: 4.116147604747839\n",
      "Epoch: 143, loss: 4.116044644633351\n",
      "Epoch: 144, loss: 4.115943719319696\n",
      "Epoch: 145, loss: 4.115844775843051\n",
      "Epoch: 146, loss: 4.115747762811282\n",
      "Epoch: 147, loss: 4.115652630352947\n",
      "Epoch: 148, loss: 4.11555933006809\n",
      "Epoch: 149, loss: 4.115467814980724\n",
      "Epoch: 150, loss: 4.115378039492982\n",
      "Epoch: 151, loss: 4.115289959340837\n",
      "Epoch: 152, loss: 4.115203531551347\n",
      "Epoch: 153, loss: 4.1151187144013734\n",
      "Epoch: 154, loss: 4.115035467377724\n",
      "Epoch: 155, loss: 4.1149537511386365\n",
      "Epoch: 156, loss: 4.114873527476599\n",
      "Epoch: 157, loss: 4.114794759282422\n",
      "Epoch: 158, loss: 4.11471741051055\n",
      "Epoch: 159, loss: 4.114641446145515\n",
      "Epoch: 160, loss: 4.114566832169562\n",
      "Epoch: 161, loss: 4.11449353553134\n",
      "Epoch: 162, loss: 4.114421524115663\n",
      "Epoch: 163, loss: 4.114350766714276\n",
      "Epoch: 164, loss: 4.114281232997608\n",
      "Epoch: 165, loss: 4.114212893487459\n",
      "Epoch: 166, loss: 4.114145719530622\n",
      "Epoch: 167, loss: 4.114079683273354\n",
      "Epoch: 168, loss: 4.114014757636707\n",
      "Epoch: 169, loss: 4.113950916292691\n",
      "Epoch: 170, loss: 4.113888133641198\n",
      "Epoch: 171, loss: 4.113826384787714\n",
      "Epoch: 172, loss: 4.113765645521752\n",
      "Epoch: 173, loss: 4.113705892295983\n",
      "Epoch: 174, loss: 4.11364710220608\n",
      "Epoch: 175, loss: 4.113589252971198\n",
      "Epoch: 176, loss: 4.113532322915098\n",
      "Epoch: 177, loss: 4.11347629094789\n",
      "Epoch: 178, loss: 4.113421136548369\n",
      "Epoch: 179, loss: 4.113366839746911\n",
      "Epoch: 180, loss: 4.113313381108951\n",
      "Epoch: 181, loss: 4.113260741718968\n",
      "Epoch: 182, loss: 4.113208903165\n",
      "Epoch: 183, loss: 4.113157847523651\n",
      "Epoch: 184, loss: 4.113107557345592\n",
      "Epoch: 185, loss: 4.113058015641502\n",
      "Epoch: 186, loss: 4.113009205868496\n",
      "Epoch: 187, loss: 4.1129611119169445\n",
      "Epoch: 188, loss: 4.112913718097746\n",
      "Epoch: 189, loss: 4.11286700912999\n",
      "Epoch: 190, loss: 4.11282097012901\n",
      "Epoch: 191, loss: 4.112775586594819\n",
      "Epoch: 192, loss: 4.11273084440091\n",
      "Epoch: 193, loss: 4.112686729783412\n",
      "Epoch: 194, loss: 4.1126432293305815\n",
      "Epoch: 195, loss: 4.112600329972621\n",
      "Epoch: 196, loss: 4.112558018971838\n",
      "Epoch: 197, loss: 4.112516283913084\n",
      "Epoch: 198, loss: 4.112475112694518\n",
      "Epoch: 199, loss: 4.1124344935186175\n",
      "Epoch: 200, loss: 4.112394414883532\n",
      "Epoch: 201, loss: 4.112354865574644\n",
      "Epoch: 202, loss: 4.1123158346564255\n",
      "Epoch: 203, loss: 4.112277311464531\n",
      "Epoch: 204, loss: 4.112239285598154\n",
      "Epoch: 205, loss: 4.112201746912586\n",
      "Epoch: 206, loss: 4.112164685512042\n",
      "Epoch: 207, loss: 4.11212809174267\n",
      "Epoch: 208, loss: 4.112091956185808\n",
      "Epoch: 209, loss: 4.112056269651416\n",
      "Epoch: 210, loss: 4.112021023171733\n",
      "Epoch: 211, loss: 4.111986207995115\n",
      "Epoch: 212, loss: 4.111951815580051\n",
      "Epoch: 213, loss: 4.111917837589391\n",
      "Epoch: 214, loss: 4.111884265884701\n",
      "Epoch: 215, loss: 4.111851092520847\n",
      "Epoch: 216, loss: 4.111818309740675\n",
      "Epoch: 217, loss: 4.111785909969923\n",
      "Epoch: 218, loss: 4.111753885812218\n",
      "Epoch: 219, loss: 4.111722230044267\n",
      "Epoch: 220, loss: 4.11169093561118\n",
      "Epoch: 221, loss: 4.111659995621923\n",
      "Epoch: 222, loss: 4.11162940334492\n",
      "Epoch: 223, loss: 4.111599152203769\n",
      "Epoch: 224, loss: 4.1115692357731\n",
      "Epoch: 225, loss: 4.111539647774555\n",
      "Epoch: 226, loss: 4.111510382072874\n",
      "Epoch: 227, loss: 4.111481432672096\n",
      "Epoch: 228, loss: 4.111452793711901\n",
      "Epoch: 229, loss: 4.111424459464014\n",
      "Epoch: 230, loss: 4.111396424328744\n",
      "Epoch: 231, loss: 4.111368682831621\n",
      "Epoch: 232, loss: 4.111341229620127\n",
      "Epoch: 233, loss: 4.11131405946051\n",
      "Epoch: 234, loss: 4.1112871672347175\n",
      "Epoch: 235, loss: 4.111260547937388\n",
      "Epoch: 236, loss: 4.111234196672952\n",
      "Epoch: 237, loss: 4.1112081086528\n",
      "Epoch: 238, loss: 4.111182279192559\n",
      "Epoch: 239, loss: 4.111156703709408\n",
      "Epoch: 240, loss: 4.111131377719493\n",
      "Epoch: 241, loss: 4.111106296835425\n",
      "Epoch: 242, loss: 4.111081456763823\n",
      "Epoch: 243, loss: 4.11105685330295\n",
      "Epoch: 244, loss: 4.111032482340402\n",
      "Epoch: 245, loss: 4.1110083398508666\n",
      "Epoch: 246, loss: 4.110984421893953\n",
      "Epoch: 247, loss: 4.11096072461206\n",
      "Epoch: 248, loss: 4.1109372442283325\n",
      "Epoch: 249, loss: 4.110913977044655\n",
      "Epoch: 250, loss: 4.110890919439709\n",
      "Epoch: 251, loss: 4.110868067867083\n",
      "Epoch: 252, loss: 4.110845418853433\n",
      "Epoch: 253, loss: 4.110822968996706\n",
      "Epoch: 254, loss: 4.110800714964385\n",
      "Epoch: 255, loss: 4.110778653491817\n",
      "Epoch: 256, loss: 4.110756781380556\n",
      "Epoch: 257, loss: 4.110735095496782\n",
      "Epoch: 258, loss: 4.1107135927697245\n",
      "Epoch: 259, loss: 4.110692270190182\n",
      "Epoch: 260, loss: 4.110671124809009\n",
      "Epoch: 261, loss: 4.110650153735734\n",
      "Epoch: 262, loss: 4.110629354137133\n",
      "Epoch: 263, loss: 4.1106087232358846\n",
      "Epoch: 264, loss: 4.110588258309258\n",
      "Epoch: 265, loss: 4.11056795668783\n",
      "Epoch: 266, loss: 4.11054781575423\n",
      "Epoch: 267, loss: 4.1105278329419335\n",
      "Epoch: 268, loss: 4.110508005734075\n",
      "Epoch: 269, loss: 4.110488331662298\n",
      "Epoch: 270, loss: 4.110468808305644\n",
      "Epoch: 271, loss: 4.11044943328945\n",
      "Epoch: 272, loss: 4.1104302042842935\n",
      "Epoch: 273, loss: 4.110411119004955\n",
      "Epoch: 274, loss: 4.110392175209419\n",
      "Epoch: 275, loss: 4.110373370697885\n",
      "Epoch: 276, loss: 4.110354703311825\n",
      "Epoch: 277, loss: 4.110336170933038\n",
      "Epoch: 278, loss: 4.110317771482756\n",
      "Epoch: 279, loss: 4.110299502920762\n",
      "Epoch: 280, loss: 4.110281363244524\n",
      "Epoch: 281, loss: 4.110263350488366\n",
      "Epoch: 282, loss: 4.110245462722647\n",
      "Epoch: 283, loss: 4.110227698052969\n",
      "Epoch: 284, loss: 4.110210054619399\n",
      "Epoch: 285, loss: 4.1101925305957225\n",
      "Epoch: 286, loss: 4.1101751241887\n",
      "Epoch: 287, loss: 4.11015783363735\n",
      "Epoch: 288, loss: 4.110140657212259\n",
      "Epoch: 289, loss: 4.110123593214895\n",
      "Epoch: 290, loss: 4.110106639976939\n",
      "Epoch: 291, loss: 4.110089795859647\n",
      "Epoch: 292, loss: 4.110073059253211\n",
      "Epoch: 293, loss: 4.110056428576148\n",
      "Epoch: 294, loss: 4.110039902274698\n",
      "Epoch: 295, loss: 4.110023478822244\n",
      "Epoch: 296, loss: 4.110007156718728\n",
      "Epoch: 297, loss: 4.109990934490112\n",
      "Epoch: 298, loss: 4.109974810687819\n",
      "Epoch: 299, loss: 4.109958783888208\n",
      "Epoch: 300, loss: 4.109942852692068\n",
      "Epoch: 301, loss: 4.109927015724093\n",
      "Epoch: 302, loss: 4.109911271632414\n",
      "Epoch: 303, loss: 4.109895619088093\n",
      "Epoch: 304, loss: 4.109880056784676\n",
      "Epoch: 305, loss: 4.109864583437721\n",
      "Epoch: 306, loss: 4.109849197784364\n",
      "Epoch: 307, loss: 4.109833898582871\n",
      "Epoch: 308, loss: 4.109818684612222\n",
      "Epoch: 309, loss: 4.109803554671685\n",
      "Epoch: 310, loss: 4.109788507580427\n",
      "Epoch: 311, loss: 4.1097735421770984\n",
      "Epoch: 312, loss: 4.109758657319463\n",
      "Epoch: 313, loss: 4.109743851884009\n",
      "Epoch: 314, loss: 4.109729124765587\n",
      "Epoch: 315, loss: 4.109714474877048\n",
      "Epoch: 316, loss: 4.109699901148887\n",
      "Epoch: 317, loss: 4.109685402528907\n",
      "Epoch: 318, loss: 4.109670977981877\n",
      "Epoch: 319, loss: 4.1096566264892065\n",
      "Epoch: 320, loss: 4.109642347048627\n",
      "Epoch: 321, loss: 4.109628138673869\n",
      "Epoch: 322, loss: 4.109614000394371\n",
      "Epoch: 323, loss: 4.109599931254973\n",
      "Epoch: 324, loss: 4.109585930315612\n",
      "Epoch: 325, loss: 4.109571996651063\n",
      "Epoch: 326, loss: 4.109558129350631\n",
      "Epoch: 327, loss: 4.109544327517896\n",
      "Epoch: 328, loss: 4.109530590270446\n",
      "Epoch: 329, loss: 4.109516916739599\n",
      "Epoch: 330, loss: 4.10950330607017\n",
      "Epoch: 331, loss: 4.109489757420202\n",
      "Epoch: 332, loss: 4.109476269960742\n",
      "Epoch: 333, loss: 4.109462842875574\n",
      "Epoch: 334, loss: 4.109449475361015\n",
      "Epoch: 335, loss: 4.109436166625667\n",
      "Epoch: 336, loss: 4.109422915890201\n",
      "Epoch: 337, loss: 4.109409722387136\n",
      "Epoch: 338, loss: 4.109396585360629\n",
      "Epoch: 339, loss: 4.109383504066253\n",
      "Epoch: 340, loss: 4.109370477770812\n",
      "Epoch: 341, loss: 4.1093575057521265\n",
      "Epoch: 342, loss: 4.109344587298836\n",
      "Epoch: 343, loss: 4.109331721710216\n",
      "Epoch: 344, loss: 4.109318908295982\n",
      "Epoch: 345, loss: 4.109306146376109\n",
      "Epoch: 346, loss: 4.109293435280651\n",
      "Epoch: 347, loss: 4.109280774349564\n",
      "Epoch: 348, loss: 4.109268162932535\n",
      "Epoch: 349, loss: 4.109255600388808\n",
      "Epoch: 350, loss: 4.109243086087027\n",
      "Epoch: 351, loss: 4.109230619405061\n",
      "Epoch: 352, loss: 4.109218199729852\n",
      "Epoch: 353, loss: 4.109205826457266\n",
      "Epoch: 354, loss: 4.109193498991932\n",
      "Epoch: 355, loss: 4.10918121674709\n",
      "Epoch: 356, loss: 4.10916897914445\n",
      "Epoch: 357, loss: 4.109156785614054\n",
      "Epoch: 358, loss: 4.109144635594121\n",
      "Epoch: 359, loss: 4.10913252853092\n",
      "Epoch: 360, loss: 4.109120463878638\n",
      "Epoch: 361, loss: 4.109108441099233\n",
      "Epoch: 362, loss: 4.1090964596623145\n",
      "Epoch: 363, loss: 4.109084519045023\n",
      "Epoch: 364, loss: 4.10907261873189\n",
      "Epoch: 365, loss: 4.109060758214731\n",
      "Epoch: 366, loss: 4.109048936992511\n",
      "Epoch: 367, loss: 4.1090371545712365\n",
      "Epoch: 368, loss: 4.109025410463841\n",
      "Epoch: 369, loss: 4.109013704190067\n",
      "Epoch: 370, loss: 4.1090020352763625\n",
      "Epoch: 371, loss: 4.108990403255759\n",
      "Epoch: 372, loss: 4.108978807667784\n",
      "Epoch: 373, loss: 4.108967248058335\n",
      "Epoch: 374, loss: 4.1089557239796015\n",
      "Epoch: 375, loss: 4.108944234989941\n",
      "Epoch: 376, loss: 4.108932780653799\n",
      "Epoch: 377, loss: 4.108921360541595\n",
      "Epoch: 378, loss: 4.1089099742296415\n",
      "Epoch: 379, loss: 4.108898621300053\n",
      "Epoch: 380, loss: 4.1088873013406335\n",
      "Epoch: 381, loss: 4.108876013944812\n",
      "Epoch: 382, loss: 4.108864758711542\n",
      "Epoch: 383, loss: 4.108853535245212\n",
      "Epoch: 384, loss: 4.108842343155566\n",
      "Epoch: 385, loss: 4.108831182057629\n",
      "Epoch: 386, loss: 4.1088200515716045\n",
      "Epoch: 387, loss: 4.10880895132281\n",
      "Epoch: 388, loss: 4.108797880941592\n",
      "Epoch: 389, loss: 4.108786840063258\n",
      "Epoch: 390, loss: 4.108775828327985\n",
      "Epoch: 391, loss: 4.108764845380757\n",
      "Epoch: 392, loss: 4.108753890871287\n",
      "Epoch: 393, loss: 4.1087429644539455\n",
      "Epoch: 394, loss: 4.1087320657876925\n",
      "Epoch: 395, loss: 4.108721194536004\n",
      "Epoch: 396, loss: 4.108710350366805\n",
      "Epoch: 397, loss: 4.108699532952408\n",
      "Epoch: 398, loss: 4.108688741969438\n",
      "Epoch: 399, loss: 4.108677977098775\n",
      "Epoch: 400, loss: 4.108667238025486\n",
      "Epoch: 401, loss: 4.10865652443877\n",
      "Epoch: 402, loss: 4.108645836031885\n",
      "Epoch: 403, loss: 4.108635172502101\n",
      "Epoch: 404, loss: 4.1086245335506275\n",
      "Epoch: 405, loss: 4.108613918882567\n",
      "Epoch: 406, loss: 4.1086033282068515\n",
      "Epoch: 407, loss: 4.108592761236185\n",
      "Epoch: 408, loss: 4.1085822176869975\n",
      "Epoch: 409, loss: 4.108571697279378\n",
      "Epoch: 410, loss: 4.108561199737028\n",
      "Epoch: 411, loss: 4.108550724787214\n",
      "Epoch: 412, loss: 4.108540272160706\n",
      "Epoch: 413, loss: 4.1085298415917295\n",
      "Epoch: 414, loss: 4.108519432817925\n",
      "Epoch: 415, loss: 4.108509045580279\n",
      "Epoch: 416, loss: 4.1084986796231\n",
      "Epoch: 417, loss: 4.108488334693956\n",
      "Epoch: 418, loss: 4.1084780105436245\n",
      "Epoch: 419, loss: 4.108467706926066\n",
      "Epoch: 420, loss: 4.108457423598358\n",
      "Epoch: 421, loss: 4.108447160320656\n",
      "Epoch: 422, loss: 4.10843691685616\n",
      "Epoch: 423, loss: 4.108426692971062\n",
      "Epoch: 424, loss: 4.108416488434507\n",
      "Epoch: 425, loss: 4.108406303018546\n",
      "Epoch: 426, loss: 4.1083961364981105\n",
      "Epoch: 427, loss: 4.108385988650949\n",
      "Epoch: 428, loss: 4.108375859257608\n",
      "Epoch: 429, loss: 4.1083657481013836\n",
      "Epoch: 430, loss: 4.108355654968288\n",
      "Epoch: 431, loss: 4.1083455796470005\n",
      "Epoch: 432, loss: 4.108335521928848\n",
      "Epoch: 433, loss: 4.1083254816077535\n",
      "Epoch: 434, loss: 4.1083154584802095\n",
      "Epoch: 435, loss: 4.108305452345237\n",
      "Epoch: 436, loss: 4.1082954630043504\n",
      "Epoch: 437, loss: 4.108285490261536\n",
      "Epoch: 438, loss: 4.108275533923189\n",
      "Epoch: 439, loss: 4.108265593798123\n",
      "Epoch: 440, loss: 4.108255669697496\n",
      "Epoch: 441, loss: 4.108245761434801\n",
      "Epoch: 442, loss: 4.108235868825832\n",
      "Epoch: 443, loss: 4.108225991688652\n",
      "Epoch: 444, loss: 4.108216129843549\n",
      "Epoch: 445, loss: 4.108206283113033\n",
      "Epoch: 446, loss: 4.108196451321778\n",
      "Epoch: 447, loss: 4.1081866342966125\n",
      "Epoch: 448, loss: 4.108176831866482\n",
      "Epoch: 449, loss: 4.108167043862421\n",
      "Epoch: 450, loss: 4.108157270117525\n",
      "Epoch: 451, loss: 4.108147510466928\n",
      "Epoch: 452, loss: 4.1081377647477675\n",
      "Epoch: 453, loss: 4.108128032799165\n",
      "Epoch: 454, loss: 4.108118314462192\n",
      "Epoch: 455, loss: 4.108108609579855\n",
      "Epoch: 456, loss: 4.108098917997058\n",
      "Epoch: 457, loss: 4.108089239560587\n",
      "Epoch: 458, loss: 4.108079574119077\n",
      "Epoch: 459, loss: 4.108069921522995\n",
      "Epoch: 460, loss: 4.10806028162461\n",
      "Epoch: 461, loss: 4.108050654277974\n",
      "Epoch: 462, loss: 4.108041039338899\n",
      "Epoch: 463, loss: 4.108031436664926\n",
      "Epoch: 464, loss: 4.108021846115308\n",
      "Epoch: 465, loss: 4.108012267550995\n",
      "Epoch: 466, loss: 4.108002700834598\n",
      "Epoch: 467, loss: 4.1079931458303784\n",
      "Epoch: 468, loss: 4.107983602404219\n",
      "Epoch: 469, loss: 4.107974070423603\n",
      "Epoch: 470, loss: 4.107964549757609\n",
      "Epoch: 471, loss: 4.107955040276865\n",
      "Epoch: 472, loss: 4.107945541853544\n",
      "Epoch: 473, loss: 4.107936054361345\n",
      "Epoch: 474, loss: 4.107926577675473\n",
      "Epoch: 475, loss: 4.107917111672605\n",
      "Epoch: 476, loss: 4.107907656230893\n",
      "Epoch: 477, loss: 4.107898211229932\n",
      "Epoch: 478, loss: 4.107888776550741\n",
      "Epoch: 479, loss: 4.107879352075753\n",
      "Epoch: 480, loss: 4.10786993768879\n",
      "Epoch: 481, loss: 4.107860533275045\n",
      "Epoch: 482, loss: 4.107851138721072\n",
      "Epoch: 483, loss: 4.107841753914762\n",
      "Epoch: 484, loss: 4.107832378745323\n",
      "Epoch: 485, loss: 4.1078230131032765\n",
      "Epoch: 486, loss: 4.107813656880425\n",
      "Epoch: 487, loss: 4.107804309969849\n",
      "Epoch: 488, loss: 4.107794972265883\n",
      "Epoch: 489, loss: 4.107785643664101\n",
      "Epoch: 490, loss: 4.107776324061306\n",
      "Epoch: 491, loss: 4.107767013355503\n",
      "Epoch: 492, loss: 4.107757711445902\n",
      "Epoch: 493, loss: 4.107748418232885\n",
      "Epoch: 494, loss: 4.107739133617997\n",
      "Epoch: 495, loss: 4.10772985750394\n",
      "Epoch: 496, loss: 4.107720589794551\n",
      "Epoch: 497, loss: 4.107711330394779\n",
      "Epoch: 498, loss: 4.107702079210696\n",
      "Epoch: 499, loss: 4.1076928361494565\n",
      "Epoch: 500, loss: 4.1076836011193\n",
      "Epoch: 501, loss: 4.107674374029533\n",
      "Epoch: 502, loss: 4.107665154790514\n",
      "Epoch: 503, loss: 4.1076559433136435\n",
      "Epoch: 504, loss: 4.1076467395113525\n",
      "Epoch: 505, loss: 4.107637543297084\n",
      "Epoch: 506, loss: 4.107628354585285\n",
      "Epoch: 507, loss: 4.1076191732913925\n",
      "Epoch: 508, loss: 4.107609999331823\n",
      "Epoch: 509, loss: 4.107600832623956\n",
      "Epoch: 510, loss: 4.107591673086135\n",
      "Epoch: 511, loss: 4.107582520637633\n",
      "Epoch: 512, loss: 4.107573375198665\n",
      "Epoch: 513, loss: 4.10756423669036\n",
      "Epoch: 514, loss: 4.107555105034757\n",
      "Epoch: 515, loss: 4.107545980154798\n",
      "Epoch: 516, loss: 4.107536861974303\n",
      "Epoch: 517, loss: 4.1075277504179715\n",
      "Epoch: 518, loss: 4.107518645411371\n",
      "Epoch: 519, loss: 4.107509546880924\n",
      "Epoch: 520, loss: 4.107500454753892\n",
      "Epoch: 521, loss: 4.107491368958375\n",
      "Epoch: 522, loss: 4.1074822894233\n",
      "Epoch: 523, loss: 4.107473216078397\n",
      "Epoch: 524, loss: 4.1074641488542145\n",
      "Epoch: 525, loss: 4.1074550876820854\n",
      "Epoch: 526, loss: 4.107446032494134\n",
      "Epoch: 527, loss: 4.107436983223261\n",
      "Epoch: 528, loss: 4.1074279398031255\n",
      "Epoch: 529, loss: 4.107418902168149\n",
      "Epoch: 530, loss: 4.107409870253505\n",
      "Epoch: 531, loss: 4.107400843995097\n",
      "Epoch: 532, loss: 4.107391823329569\n",
      "Epoch: 533, loss: 4.107382808194278\n",
      "Epoch: 534, loss: 4.107373798527301\n",
      "Epoch: 535, loss: 4.107364794267415\n",
      "Epoch: 536, loss: 4.107355795354093\n",
      "Epoch: 537, loss: 4.1073468017274966\n",
      "Epoch: 538, loss: 4.1073378133284715\n",
      "Epoch: 539, loss: 4.1073288300985284\n",
      "Epoch: 540, loss: 4.107319851979845\n",
      "Epoch: 541, loss: 4.107310878915254\n",
      "Epoch: 542, loss: 4.1073019108482365\n",
      "Epoch: 543, loss: 4.107292947722912\n",
      "Epoch: 544, loss: 4.107283989484037\n",
      "Epoch: 545, loss: 4.107275036076988\n",
      "Epoch: 546, loss: 4.107266087447765\n",
      "Epoch: 547, loss: 4.107257143542971\n",
      "Epoch: 548, loss: 4.107248204309821\n",
      "Epoch: 549, loss: 4.107239269696122\n",
      "Epoch: 550, loss: 4.107230339650268\n",
      "Epoch: 551, loss: 4.107221414121243\n",
      "Epoch: 552, loss: 4.1072124930585945\n",
      "Epoch: 553, loss: 4.107203576412454\n",
      "Epoch: 554, loss: 4.107194664133503\n",
      "Epoch: 555, loss: 4.107185756172978\n",
      "Epoch: 556, loss: 4.1071768524826755\n",
      "Epoch: 557, loss: 4.107167953014928\n",
      "Epoch: 558, loss: 4.107159057722603\n",
      "Epoch: 559, loss: 4.107150166559099\n",
      "Epoch: 560, loss: 4.10714127947834\n",
      "Epoch: 561, loss: 4.107132396434767\n",
      "Epoch: 562, loss: 4.107123517383332\n",
      "Epoch: 563, loss: 4.107114642279492\n",
      "Epoch: 564, loss: 4.10710577107921\n",
      "Epoch: 565, loss: 4.107096903738933\n",
      "Epoch: 566, loss: 4.107088040215606\n",
      "Epoch: 567, loss: 4.1070791804666475\n",
      "Epoch: 568, loss: 4.107070324449962\n",
      "Epoch: 569, loss: 4.107061472123924\n",
      "Epoch: 570, loss: 4.107052623447368\n",
      "Epoch: 571, loss: 4.107043778379594\n",
      "Epoch: 572, loss: 4.107034936880357\n",
      "Epoch: 573, loss: 4.107026098909862\n",
      "Epoch: 574, loss: 4.1070172644287615\n",
      "Epoch: 575, loss: 4.107008433398141\n",
      "Epoch: 576, loss: 4.106999605779525\n",
      "Epoch: 577, loss: 4.106990781534876\n",
      "Epoch: 578, loss: 4.106981960626564\n",
      "Epoch: 579, loss: 4.1069731430173935\n",
      "Epoch: 580, loss: 4.106964328670579\n",
      "Epoch: 581, loss: 4.106955517549743\n",
      "Epoch: 582, loss: 4.10694670961892\n",
      "Epoch: 583, loss: 4.10693790484254\n",
      "Epoch: 584, loss: 4.10692910318543\n",
      "Epoch: 585, loss: 4.106920304612814\n",
      "Epoch: 586, loss: 4.106911509090298\n",
      "Epoch: 587, loss: 4.106902716583873\n",
      "Epoch: 588, loss: 4.106893927059909\n",
      "Epoch: 589, loss: 4.106885140485153\n",
      "Epoch: 590, loss: 4.106876356826714\n",
      "Epoch: 591, loss: 4.106867576052075\n",
      "Epoch: 592, loss: 4.106858798129078\n",
      "Epoch: 593, loss: 4.106850023025924\n",
      "Epoch: 594, loss: 4.106841250711163\n",
      "Epoch: 595, loss: 4.106832481153701\n",
      "Epoch: 596, loss: 4.106823714322786\n",
      "Epoch: 597, loss: 4.1068149501880065\n",
      "Epoch: 598, loss: 4.106806188719286\n",
      "Epoch: 599, loss: 4.106797429886895\n",
      "Epoch: 600, loss: 4.106788673661422\n",
      "Epoch: 601, loss: 4.106779920013787\n",
      "Epoch: 602, loss: 4.106771168915227\n",
      "Epoch: 603, loss: 4.106762420337306\n",
      "Epoch: 604, loss: 4.106753674251899\n",
      "Epoch: 605, loss: 4.106744930631195\n",
      "Epoch: 606, loss: 4.1067361894476875\n",
      "Epoch: 607, loss: 4.10672745067418\n",
      "Epoch: 608, loss: 4.106718714283778\n",
      "Epoch: 609, loss: 4.106709980249879\n",
      "Epoch: 610, loss: 4.10670124854618\n",
      "Epoch: 611, loss: 4.10669251914667\n",
      "Epoch: 612, loss: 4.106683792025621\n",
      "Epoch: 613, loss: 4.106675067157597\n",
      "Epoch: 614, loss: 4.106666344517439\n",
      "Epoch: 615, loss: 4.106657624080269\n",
      "Epoch: 616, loss: 4.106648905821483\n",
      "Epoch: 617, loss: 4.106640189716751\n",
      "Epoch: 618, loss: 4.106631475742008\n",
      "Epoch: 619, loss: 4.106622763873463\n",
      "Epoch: 620, loss: 4.106614054087583\n",
      "Epoch: 621, loss: 4.106605346361094\n",
      "Epoch: 622, loss: 4.106596640670986\n",
      "Epoch: 623, loss: 4.106587936994497\n",
      "Epoch: 624, loss: 4.106579235309125\n",
      "Epoch: 625, loss: 4.106570535592601\n",
      "Epoch: 626, loss: 4.106561837822924\n",
      "Epoch: 627, loss: 4.106553141978319\n",
      "Epoch: 628, loss: 4.106544448037258\n",
      "Epoch: 629, loss: 4.106535755978457\n",
      "Epoch: 630, loss: 4.106527065780855\n",
      "Epoch: 631, loss: 4.106518377423636\n",
      "Epoch: 632, loss: 4.106509690886204\n",
      "Epoch: 633, loss: 4.1065010061482\n",
      "Epoch: 634, loss: 4.106492323189482\n",
      "Epoch: 635, loss: 4.106483641990139\n",
      "Epoch: 636, loss: 4.10647496253047\n",
      "Epoch: 637, loss: 4.106466284791002\n",
      "Epoch: 638, loss: 4.10645760875247\n",
      "Epoch: 639, loss: 4.106448934395826\n",
      "Epoch: 640, loss: 4.1064402617022315\n",
      "Epoch: 641, loss: 4.106431590653057\n",
      "Epoch: 642, loss: 4.106422921229876\n",
      "Epoch: 643, loss: 4.106414253414469\n",
      "Epoch: 644, loss: 4.106405587188817\n",
      "Epoch: 645, loss: 4.106396922535103\n",
      "Epoch: 646, loss: 4.106388259435696\n",
      "Epoch: 647, loss: 4.106379597873177\n",
      "Epoch: 648, loss: 4.106370937830307\n",
      "Epoch: 649, loss: 4.106362279290041\n",
      "Epoch: 650, loss: 4.106353622235522\n",
      "Epoch: 651, loss: 4.1063449666500835\n",
      "Epoch: 652, loss: 4.106336312517238\n",
      "Epoch: 653, loss: 4.106327659820683\n",
      "Epoch: 654, loss: 4.106319008544294\n",
      "Epoch: 655, loss: 4.106310358672131\n",
      "Epoch: 656, loss: 4.106301710188422\n",
      "Epoch: 657, loss: 4.106293063077579\n",
      "Epoch: 658, loss: 4.106284417324175\n",
      "Epoch: 659, loss: 4.106275772912963\n",
      "Epoch: 660, loss: 4.106267129828862\n",
      "Epoch: 661, loss: 4.106258488056956\n",
      "Epoch: 662, loss: 4.106249847582498\n",
      "Epoch: 663, loss: 4.106241208390901\n",
      "Epoch: 664, loss: 4.106232570467737\n",
      "Epoch: 665, loss: 4.106223933798748\n",
      "Epoch: 666, loss: 4.106215298369821\n",
      "Epoch: 667, loss: 4.1062066641670105\n",
      "Epoch: 668, loss: 4.106198031176519\n",
      "Epoch: 669, loss: 4.1061893993847045\n",
      "Epoch: 670, loss: 4.1061807687780725\n",
      "Epoch: 671, loss: 4.1061721393432835\n",
      "Epoch: 672, loss: 4.106163511067141\n",
      "Epoch: 673, loss: 4.1061548839366\n",
      "Epoch: 674, loss: 4.1061462579387555\n",
      "Epoch: 675, loss: 4.106137633060849\n",
      "Epoch: 676, loss: 4.106129009290258\n",
      "Epoch: 677, loss: 4.106120386614507\n",
      "Epoch: 678, loss: 4.106111765021256\n",
      "Epoch: 679, loss: 4.106103144498303\n",
      "Epoch: 680, loss: 4.106094525033581\n",
      "Epoch: 681, loss: 4.106085906615154\n",
      "Epoch: 682, loss: 4.106077289231223\n",
      "Epoch: 683, loss: 4.106068672870116\n",
      "Epoch: 684, loss: 4.106060057520297\n",
      "Epoch: 685, loss: 4.1060514431703545\n",
      "Epoch: 686, loss: 4.106042829808999\n",
      "Epoch: 687, loss: 4.1060342174250755\n",
      "Epoch: 688, loss: 4.106025606007548\n",
      "Epoch: 689, loss: 4.106016995545506\n",
      "Epoch: 690, loss: 4.106008386028154\n",
      "Epoch: 691, loss: 4.105999777444826\n",
      "Epoch: 692, loss: 4.105991169784969\n",
      "Epoch: 693, loss: 4.1059825630381495\n",
      "Epoch: 694, loss: 4.10597395719405\n",
      "Epoch: 695, loss: 4.1059653522424675\n",
      "Epoch: 696, loss: 4.105956748173312\n",
      "Epoch: 697, loss: 4.105948144976609\n",
      "Epoch: 698, loss: 4.105939542642493\n",
      "Epoch: 699, loss: 4.10593094116121\n",
      "Epoch: 700, loss: 4.105922340523113\n",
      "Epoch: 701, loss: 4.105913740718666\n",
      "Epoch: 702, loss: 4.105905141738434\n",
      "Epoch: 703, loss: 4.105896543573095\n",
      "Epoch: 704, loss: 4.105887946213426\n",
      "Epoch: 705, loss: 4.105879349650308\n",
      "Epoch: 706, loss: 4.105870753874723\n",
      "Epoch: 707, loss: 4.10586215887776\n",
      "Epoch: 708, loss: 4.105853564650602\n",
      "Epoch: 709, loss: 4.105844971184532\n",
      "Epoch: 710, loss: 4.105836378470928\n",
      "Epoch: 711, loss: 4.105827786501273\n",
      "Epoch: 712, loss: 4.105819195267136\n",
      "Epoch: 713, loss: 4.1058106047601886\n",
      "Epoch: 714, loss: 4.10580201497219\n",
      "Epoch: 715, loss: 4.105793425894996\n",
      "Epoch: 716, loss: 4.105784837520554\n",
      "Epoch: 717, loss: 4.105776249840896\n",
      "Epoch: 718, loss: 4.10576766284815\n",
      "Epoch: 719, loss: 4.105759076534533\n",
      "Epoch: 720, loss: 4.105750490892344\n",
      "Epoch: 721, loss: 4.105741905913974\n",
      "Epoch: 722, loss: 4.105733321591898\n",
      "Epoch: 723, loss: 4.105724737918676\n",
      "Epoch: 724, loss: 4.10571615488695\n",
      "Epoch: 725, loss: 4.10570757248945\n",
      "Epoch: 726, loss: 4.105698990718981\n",
      "Epoch: 727, loss: 4.105690409568441\n",
      "Epoch: 728, loss: 4.105681829030792\n",
      "Epoch: 729, loss: 4.105673249099089\n",
      "Epoch: 730, loss: 4.105664669766461\n",
      "Epoch: 731, loss: 4.105656091026118\n",
      "Epoch: 732, loss: 4.105647512871337\n",
      "Epoch: 733, loss: 4.105638935295482\n",
      "Epoch: 734, loss: 4.105630358291989\n",
      "Epoch: 735, loss: 4.105621781854366\n",
      "Epoch: 736, loss: 4.105613205976201\n",
      "Epoch: 737, loss: 4.10560463065115\n",
      "Epoch: 738, loss: 4.10559605587294\n",
      "Epoch: 739, loss: 4.105587481635371\n",
      "Epoch: 740, loss: 4.1055789079323155\n",
      "Epoch: 741, loss: 4.105570334757716\n",
      "Epoch: 742, loss: 4.105561762105581\n",
      "Epoch: 743, loss: 4.10555318996999\n",
      "Epoch: 744, loss: 4.105544618345087\n",
      "Epoch: 745, loss: 4.1055360472250895\n",
      "Epoch: 746, loss: 4.105527476604273\n",
      "Epoch: 747, loss: 4.105518906476984\n",
      "Epoch: 748, loss: 4.105510336837634\n",
      "Epoch: 749, loss: 4.105501767680692\n",
      "Epoch: 750, loss: 4.1054931990007\n",
      "Epoch: 751, loss: 4.105484630792257\n",
      "Epoch: 752, loss: 4.105476063050024\n",
      "Epoch: 753, loss: 4.105467495768725\n",
      "Epoch: 754, loss: 4.105458928943145\n",
      "Epoch: 755, loss: 4.105450362568126\n",
      "Epoch: 756, loss: 4.105441796638572\n",
      "Epoch: 757, loss: 4.10543323114945\n",
      "Epoch: 758, loss: 4.105424666095775\n",
      "Epoch: 759, loss: 4.105416101472629\n",
      "Epoch: 760, loss: 4.105407537275147\n",
      "Epoch: 761, loss: 4.105398973498517\n",
      "Epoch: 762, loss: 4.105390410137989\n",
      "Epoch: 763, loss: 4.105381847188868\n",
      "Epoch: 764, loss: 4.1053732846465065\n",
      "Epoch: 765, loss: 4.105364722506318\n",
      "Epoch: 766, loss: 4.105356160763765\n",
      "Epoch: 767, loss: 4.105347599414364\n",
      "Epoch: 768, loss: 4.105339038453688\n",
      "Epoch: 769, loss: 4.105330477877351\n",
      "Epoch: 770, loss: 4.105321917681033\n",
      "Epoch: 771, loss: 4.105313357860452\n",
      "Epoch: 772, loss: 4.1053047984113835\n",
      "Epoch: 773, loss: 4.105296239329648\n",
      "Epoch: 774, loss: 4.105287680611111\n",
      "Epoch: 775, loss: 4.1052791222517016\n",
      "Epoch: 776, loss: 4.105270564247384\n",
      "Epoch: 777, loss: 4.105262006594169\n",
      "Epoch: 778, loss: 4.1052534492881225\n",
      "Epoch: 779, loss: 4.10524489232535\n",
      "Epoch: 780, loss: 4.105236335702008\n",
      "Epoch: 781, loss: 4.105227779414293\n",
      "Epoch: 782, loss: 4.105219223458451\n",
      "Epoch: 783, loss: 4.105210667830769\n",
      "Epoch: 784, loss: 4.105202112527582\n",
      "Epoch: 785, loss: 4.105193557545267\n",
      "Epoch: 786, loss: 4.10518500288024\n",
      "Epoch: 787, loss: 4.105176448528963\n",
      "Epoch: 788, loss: 4.105167894487939\n",
      "Epoch: 789, loss: 4.1051593407537155\n",
      "Epoch: 790, loss: 4.10515078732288\n",
      "Epoch: 791, loss: 4.105142234192055\n",
      "Epoch: 792, loss: 4.105133681357914\n",
      "Epoch: 793, loss: 4.10512512881716\n",
      "Epoch: 794, loss: 4.105116576566541\n",
      "Epoch: 795, loss: 4.105108024602845\n",
      "Epoch: 796, loss: 4.105099472922891\n",
      "Epoch: 797, loss: 4.10509092152355\n",
      "Epoch: 798, loss: 4.105082370401714\n",
      "Epoch: 799, loss: 4.105073819554331\n",
      "Epoch: 800, loss: 4.105065268978364\n",
      "Epoch: 801, loss: 4.105056718670831\n",
      "Epoch: 802, loss: 4.10504816862878\n",
      "Epoch: 803, loss: 4.105039618849292\n",
      "Epoch: 804, loss: 4.1050310693294865\n",
      "Epoch: 805, loss: 4.10502252006652\n",
      "Epoch: 806, loss: 4.105013971057574\n",
      "Epoch: 807, loss: 4.105005422299879\n",
      "Epoch: 808, loss: 4.104996873790687\n",
      "Epoch: 809, loss: 4.10498832552729\n",
      "Epoch: 810, loss: 4.10497977750701\n",
      "Epoch: 811, loss: 4.104971229727203\n",
      "Epoch: 812, loss: 4.10496268218526\n",
      "Epoch: 813, loss: 4.104954134878599\n",
      "Epoch: 814, loss: 4.104945587804675\n",
      "Epoch: 815, loss: 4.104937040960967\n",
      "Epoch: 816, loss: 4.1049284943449935\n",
      "Epoch: 817, loss: 4.104919947954303\n",
      "Epoch: 818, loss: 4.104911401786466\n",
      "Epoch: 819, loss: 4.104902855839091\n",
      "Epoch: 820, loss: 4.104894310109813\n",
      "Epoch: 821, loss: 4.1048857645963\n",
      "Epoch: 822, loss: 4.104877219296246\n",
      "Epoch: 823, loss: 4.104868674207372\n",
      "Epoch: 824, loss: 4.10486012932743\n",
      "Epoch: 825, loss: 4.1048515846542015\n",
      "Epoch: 826, loss: 4.1048430401854965\n",
      "Epoch: 827, loss: 4.1048344959191425\n",
      "Epoch: 828, loss: 4.1048259518530115\n",
      "Epoch: 829, loss: 4.104817407984988\n",
      "Epoch: 830, loss: 4.104808864312989\n",
      "Epoch: 831, loss: 4.104800320834958\n",
      "Epoch: 832, loss: 4.104791777548866\n",
      "Epoch: 833, loss: 4.104783234452705\n",
      "Epoch: 834, loss: 4.104774691544493\n",
      "Epoch: 835, loss: 4.104766148822279\n",
      "Epoch: 836, loss: 4.104757606284135\n",
      "Epoch: 837, loss: 4.1047490639281525\n",
      "Epoch: 838, loss: 4.104740521752454\n",
      "Epoch: 839, loss: 4.104731979755181\n",
      "Epoch: 840, loss: 4.104723437934507\n",
      "Epoch: 841, loss: 4.104714896288615\n",
      "Epoch: 842, loss: 4.104706354815727\n",
      "Epoch: 843, loss: 4.104697813514075\n",
      "Epoch: 844, loss: 4.1046892723819255\n",
      "Epoch: 845, loss: 4.104680731417561\n",
      "Epoch: 846, loss: 4.104672190619287\n",
      "Epoch: 847, loss: 4.1046636499854285\n",
      "Epoch: 848, loss: 4.104655109514341\n",
      "Epoch: 849, loss: 4.104646569204394\n",
      "Epoch: 850, loss: 4.10463802905398\n",
      "Epoch: 851, loss: 4.104629489061515\n",
      "Epoch: 852, loss: 4.104620949225432\n",
      "Epoch: 853, loss: 4.104612409544191\n",
      "Epoch: 854, loss: 4.104603870016266\n",
      "Epoch: 855, loss: 4.104595330640153\n",
      "Epoch: 856, loss: 4.104586791414373\n",
      "Epoch: 857, loss: 4.104578252337461\n",
      "Epoch: 858, loss: 4.1045697134079715\n",
      "Epoch: 859, loss: 4.104561174624482\n",
      "Epoch: 860, loss: 4.104552635985593\n",
      "Epoch: 861, loss: 4.1045440974899075\n",
      "Epoch: 862, loss: 4.10453555913607\n",
      "Epoch: 863, loss: 4.104527020922723\n",
      "Epoch: 864, loss: 4.104518482848545\n",
      "Epoch: 865, loss: 4.104509944912217\n",
      "Epoch: 866, loss: 4.104501407112448\n",
      "Epoch: 867, loss: 4.104492869447962\n",
      "Epoch: 868, loss: 4.104484331917497\n",
      "Epoch: 869, loss: 4.10447579451982\n",
      "Epoch: 870, loss: 4.1044672572537\n",
      "Epoch: 871, loss: 4.1044587201179334\n",
      "Epoch: 872, loss: 4.104450183111328\n",
      "Epoch: 873, loss: 4.10444164623271\n",
      "Epoch: 874, loss: 4.104433109480926\n",
      "Epoch: 875, loss: 4.104424572854833\n",
      "Epoch: 876, loss: 4.104416036353304\n",
      "Epoch: 877, loss: 4.104407499975235\n",
      "Epoch: 878, loss: 4.104398963719531\n",
      "Epoch: 879, loss: 4.104390427585111\n",
      "Epoch: 880, loss: 4.1043818915709185\n",
      "Epoch: 881, loss: 4.104373355675901\n",
      "Epoch: 882, loss: 4.104364819899032\n",
      "Epoch: 883, loss: 4.104356284239293\n",
      "Epoch: 884, loss: 4.104347748695677\n",
      "Epoch: 885, loss: 4.104339213267204\n",
      "Epoch: 886, loss: 4.104330677952895\n",
      "Epoch: 887, loss: 4.104322142751795\n",
      "Epoch: 888, loss: 4.104313607662955\n",
      "Epoch: 889, loss: 4.104305072685446\n",
      "Epoch: 890, loss: 4.10429653781835\n",
      "Epoch: 891, loss: 4.1042880030607645\n",
      "Epoch: 892, loss: 4.104279468411797\n",
      "Epoch: 893, loss: 4.1042709338705725\n",
      "Epoch: 894, loss: 4.104262399436227\n",
      "Epoch: 895, loss: 4.104253865107908\n",
      "Epoch: 896, loss: 4.104245330884779\n",
      "Epoch: 897, loss: 4.1042367967660125\n",
      "Epoch: 898, loss: 4.104228262750798\n",
      "Epoch: 899, loss: 4.104219728838336\n",
      "Epoch: 900, loss: 4.104211195027836\n",
      "Epoch: 901, loss: 4.104202661318522\n",
      "Epoch: 902, loss: 4.10419412770963\n",
      "Epoch: 903, loss: 4.104185594200411\n",
      "Epoch: 904, loss: 4.104177060790122\n",
      "Epoch: 905, loss: 4.104168527478036\n",
      "Epoch: 906, loss: 4.104159994263434\n",
      "Epoch: 907, loss: 4.1041514611456105\n",
      "Epoch: 908, loss: 4.104142928123873\n",
      "Epoch: 909, loss: 4.104134395197536\n",
      "Epoch: 910, loss: 4.104125862365926\n",
      "Epoch: 911, loss: 4.104117329628384\n",
      "Epoch: 912, loss: 4.104108796984256\n",
      "Epoch: 913, loss: 4.104100264432904\n",
      "Epoch: 914, loss: 4.104091731973694\n",
      "Epoch: 915, loss: 4.104083199606011\n",
      "Epoch: 916, loss: 4.104074667329242\n",
      "Epoch: 917, loss: 4.104066135142793\n",
      "Epoch: 918, loss: 4.10405760304607\n",
      "Epoch: 919, loss: 4.104049071038496\n",
      "Epoch: 920, loss: 4.104040539119498\n",
      "Epoch: 921, loss: 4.10403200728852\n",
      "Epoch: 922, loss: 4.104023475545008\n",
      "Epoch: 923, loss: 4.104014943888424\n",
      "Epoch: 924, loss: 4.104006412318235\n",
      "Epoch: 925, loss: 4.1039978808339175\n",
      "Epoch: 926, loss: 4.103989349434961\n",
      "Epoch: 927, loss: 4.103980818120859\n",
      "Epoch: 928, loss: 4.103972286891118\n",
      "Epoch: 929, loss: 4.103963755745246\n",
      "Epoch: 930, loss: 4.103955224682774\n",
      "Epoch: 931, loss: 4.103946693703227\n",
      "Epoch: 932, loss: 4.103938162806143\n",
      "Epoch: 933, loss: 4.103929631991073\n",
      "Epoch: 934, loss: 4.10392110125757\n",
      "Epoch: 935, loss: 4.1039125706052015\n",
      "Epoch: 936, loss: 4.103904040033538\n",
      "Epoch: 937, loss: 4.103895509542159\n",
      "Epoch: 938, loss: 4.103886979130654\n",
      "Epoch: 939, loss: 4.103878448798617\n",
      "Epoch: 940, loss: 4.103869918545656\n",
      "Epoch: 941, loss: 4.103861388371375\n",
      "Epoch: 942, loss: 4.103852858275401\n",
      "Epoch: 943, loss: 4.103844328257356\n",
      "Epoch: 944, loss: 4.103835798316875\n",
      "Epoch: 945, loss: 4.103827268453599\n",
      "Epoch: 946, loss: 4.103818738667173\n",
      "Epoch: 947, loss: 4.103810208957257\n",
      "Epoch: 948, loss: 4.103801679323513\n",
      "Epoch: 949, loss: 4.103793149765607\n",
      "Epoch: 950, loss: 4.103784620283219\n",
      "Epoch: 951, loss: 4.103776090876028\n",
      "Epoch: 952, loss: 4.10376756154373\n",
      "Epoch: 953, loss: 4.103759032286014\n",
      "Epoch: 954, loss: 4.103750503102591\n",
      "Epoch: 955, loss: 4.10374197399316\n",
      "Epoch: 956, loss: 4.103733444957446\n",
      "Epoch: 957, loss: 4.103724915995169\n",
      "Epoch: 958, loss: 4.103716387106054\n",
      "Epoch: 959, loss: 4.103707858289838\n",
      "Epoch: 960, loss: 4.103699329546263\n",
      "Epoch: 961, loss: 4.103690800875072\n",
      "Epoch: 962, loss: 4.103682272276019\n",
      "Epoch: 963, loss: 4.103673743748864\n",
      "Epoch: 964, loss: 4.10366521529337\n",
      "Epoch: 965, loss: 4.103656686909306\n",
      "Epoch: 966, loss: 4.103648158596451\n",
      "Epoch: 967, loss: 4.103639630354582\n",
      "Epoch: 968, loss: 4.103631102183488\n",
      "Epoch: 969, loss: 4.103622574082962\n",
      "Epoch: 970, loss: 4.103614046052801\n",
      "Epoch: 971, loss: 4.103605518092808\n",
      "Epoch: 972, loss: 4.103596990202788\n",
      "Epoch: 973, loss: 4.103588462382558\n",
      "Epoch: 974, loss: 4.103579934631937\n",
      "Epoch: 975, loss: 4.103571406950742\n",
      "Epoch: 976, loss: 4.103562879338811\n",
      "Epoch: 977, loss: 4.1035543517959745\n",
      "Epoch: 978, loss: 4.103545824322066\n",
      "Epoch: 979, loss: 4.103537296916934\n",
      "Epoch: 980, loss: 4.103528769580424\n",
      "Epoch: 981, loss: 4.10352024231239\n",
      "Epoch: 982, loss: 4.10351171511269\n",
      "Epoch: 983, loss: 4.103503187981181\n",
      "Epoch: 984, loss: 4.103494660917735\n",
      "Epoch: 985, loss: 4.103486133922221\n",
      "Epoch: 986, loss: 4.103477606994514\n",
      "Epoch: 987, loss: 4.103469080134493\n",
      "Epoch: 988, loss: 4.103460553342043\n",
      "Epoch: 989, loss: 4.103452026617053\n",
      "Epoch: 990, loss: 4.103443499959415\n",
      "Epoch: 991, loss: 4.103434973369023\n",
      "Epoch: 992, loss: 4.103426446845781\n",
      "Epoch: 993, loss: 4.1034179203895915\n",
      "Epoch: 994, loss: 4.103409394000364\n",
      "Epoch: 995, loss: 4.103400867678008\n",
      "Epoch: 996, loss: 4.103392341422447\n",
      "Epoch: 997, loss: 4.103383815233595\n",
      "Epoch: 998, loss: 4.103375289111378\n",
      "Epoch: 999, loss: 4.103366763055726\n",
      "Epoch: 1000, loss: 4.103358237066567\n"
     ]
    }
   ],
   "source": [
    "# initialize weights and biases\n",
    "# in Keras/TensorFlow/PyTorch etc. these are usually randomized in the beginning\n",
    "w1 = 0.5 # starting value 1.5\n",
    "w2 = -0.5 # 0.5\n",
    "w3 = 0.5 # -2\n",
    "w4 = -0.5 # -0.5\n",
    "w5 = 0.5 # 1.5\n",
    "w6 = -0.5 # 1.2\n",
    "\n",
    "bias1 = 1  # starting value 1\n",
    "bias2 = -1 # starting value -0.35\n",
    "bias3 = 1 # starting value 0.5\n",
    "\n",
    "# just for comparison after the training\n",
    "original_w1 = w1\n",
    "original_w2 = w2\n",
    "original_w3 = w3\n",
    "original_w4 = w4\n",
    "original_w5 = w5\n",
    "original_w6 = w6\n",
    "original_b1 = bias1\n",
    "original_b2 = bias2\n",
    "original_b3 = bias3\n",
    "\n",
    "# use generated training data from our helper function\n",
    "data = generate_train_data()\n",
    "\n",
    "# learning rate\n",
    "LR = 0.0004\n",
    "epochs = 1000\n",
    "\n",
    "# let's initalize a list for loss visualizations\n",
    "loss_points = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # the previous version only measured the loss value\n",
    "    # of the last calculation done in the code (node 3)\n",
    "    # it's probably better to measure the average loss for each epoch\n",
    "    epoch_losses = []\n",
    "\n",
    "    for row in data:\n",
    "        # this is where we do Forward pass + backpropagation\n",
    "        input1 = row[0]\n",
    "        input2 = row[1]\n",
    "        true_value = row[2]\n",
    "\n",
    "        # NODE 1 OUTPUT\n",
    "        node_1_output = input1 * w1 + input2 * w3 + bias1\n",
    "        node_1_output = activation_ReLu(node_1_output)\n",
    "\n",
    "        # NODE 2 OUTPUT\n",
    "        node_2_output = input1 * w2 + input2 * w4 + bias2\n",
    "        node_2_output = activation_ReLu(node_2_output)\n",
    "\n",
    "        # NODE 3 OUTPUT\n",
    "        # we can just use Node 1 and 2 outputs, since they\n",
    "        # already contain the previous weights in their result\n",
    "        node_3_output = node_1_output * w5 + node_2_output * w6 + bias3\n",
    "        node_3_output = activation_ReLu(node_3_output)\n",
    "\n",
    "        # LOSS FUNCTION - we are going to use MSE -> mean squared error\n",
    "        # MSE formula for LOSS => (predicted_value - true_value) ^ 2\n",
    "        predicted_value = node_3_output\n",
    "        loss = (predicted_value - true_value) ** 2\n",
    "\n",
    "        # add current loss into epoch losses -list\n",
    "        epoch_losses.append(loss)\n",
    "        \n",
    "        # BACKPROPAGATION - LAST LAYER FIRST\n",
    "        # solving the partial derivative of the loss function with respect to w5\n",
    "        deriv_L_w5 = 2 * node_1_output * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_w5 = w5 - LR * deriv_L_w5\n",
    "\n",
    "        deriv_L_w6 = 2 * node_2_output * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_w6 = w6 - LR * deriv_L_w6\n",
    "\n",
    "        deriv_L_b3 = 2 * 1 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_b3 = bias3 - LR * deriv_L_b3\n",
    "\n",
    "        # BACKPROPAGATION - THE FIRST LAYER\n",
    "        # FROM THIS POINT ONWARD WE HAVE TO USE THE MORE COMPLEX VERSION\n",
    "        # OF UPDATING THE VALUES => CHAIN RULE\n",
    "\n",
    "        # weight 1\n",
    "        deriv_L_w1_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w1_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * input1\n",
    "        deriv_L_w1 = deriv_L_w1_left * deriv_L_w1_right\n",
    "        new_w1 = w1 - LR * deriv_L_w1\n",
    "\n",
    "        # weight 2\n",
    "        deriv_L_w2_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w2_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w4 + bias2) * input1\n",
    "        deriv_L_w2 = deriv_L_w2_left * deriv_L_w2_right\n",
    "        new_w2 = w2 - LR * deriv_L_w2\n",
    "\n",
    "        # weight 3\n",
    "        deriv_L_w3_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w3_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * input2\n",
    "        deriv_L_w3 = deriv_L_w3_left * deriv_L_w3_right\n",
    "        new_w3 = w3 - LR * deriv_L_w3\n",
    "\n",
    "        # weight 4\n",
    "        deriv_L_w4_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w4_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w4 + bias2) * input2\n",
    "        deriv_L_w4 = deriv_L_w4_left * deriv_L_w4_right\n",
    "        new_w4 = w4 - LR * deriv_L_w4\n",
    "\n",
    "        # bias 1\n",
    "        deriv_L_b1_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_b1_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * 1\n",
    "        deriv_L_b1 = deriv_L_b1_left * deriv_L_b1_right\n",
    "        new_b1 = bias1 - LR * deriv_L_b1\n",
    "\n",
    "        # bias 2\n",
    "        deriv_L_b2_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_b2_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w4 + bias2) * 1\n",
    "        deriv_L_b2 = deriv_L_b2_left * deriv_L_b2_right\n",
    "        new_b2 = bias2 - LR * deriv_L_b2\n",
    "\n",
    "        # ALL DONE! FINALLY UPDATE THE EXISTING WEIGHTS\n",
    "        w1 = new_w1\n",
    "        w2 = new_w2\n",
    "        w3 = new_w3\n",
    "        w4 = new_w4\n",
    "        w5 = new_w5\n",
    "        w6 = new_w6\n",
    "        bias1 = new_b1\n",
    "        bias2 = new_b2\n",
    "        bias3 = new_b3\n",
    "\n",
    "    # calculate average epoch-wise loss and add it to loss points\n",
    "    average_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "\n",
    "    # place the overall epoch loss into the loss_points list\n",
    "    loss_points.append(average_loss)\n",
    "    print(f\"Epoch: {epoch + 1}, loss: {average_loss}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_points)\n",
    "# plt.ylim(-1, 5)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL WEIGHTS AND BIASES\n",
      "w1: 0.5\n",
      "w2: -0.5\n",
      "w3: 0.5\n",
      "w4: -0.5\n",
      "w5: 0.5\n",
      "w6: -0.5\n",
      "b1: 1\n",
      "b2: -1\n",
      "b3: 1\n",
      "\n",
      "\n",
      "#################################\n",
      "\n",
      "\n",
      "NEW WEIGHTS AND BIASES\n",
      "w1: 2.40284858318998\n",
      "w2: -0.5\n",
      "w3: 0.48405702354753005\n",
      "w4: -0.5\n",
      "w5: 1.5612714318544874\n",
      "w6: -0.5\n",
      "b1: 0.05360208473832497\n",
      "b2: -1.0\n",
      "b3: 1.2274255375686283\n"
     ]
    }
   ],
   "source": [
    "print(\"ORIGINAL WEIGHTS AND BIASES\")\n",
    "print(f\"w1: {original_w1}\")\n",
    "print(f\"w2: {original_w2}\")\n",
    "print(f\"w3: {original_w3}\")\n",
    "print(f\"w4: {original_w4}\")\n",
    "print(f\"w5: {original_w5}\")\n",
    "print(f\"w6: {original_w6}\")\n",
    "print(f\"b1: {original_b1}\")\n",
    "print(f\"b2: {original_b2}\")\n",
    "print(f\"b3: {original_b3}\")\n",
    "\n",
    "print(\"\\n\\n#################################\\n\\n\")\n",
    "\n",
    "print(\"NEW WEIGHTS AND BIASES\")\n",
    "print(f\"w1: {new_w1}\")\n",
    "print(f\"w2: {new_w2}\")\n",
    "print(f\"w3: {new_w3}\")\n",
    "print(f\"w4: {new_w4}\")\n",
    "print(f\"w5: {new_w5}\")\n",
    "print(f\"w6: {new_w6}\")\n",
    "print(f\"b1: {new_b1}\")\n",
    "print(f\"b2: {new_b2}\")\n",
    "print(f\"b3: {new_b3}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction function, just doing the forward pass\n",
    "# again (but only that)\n",
    "def predict(x1, x2):\n",
    "    input1 = x1\n",
    "    input2 = x2\n",
    "\n",
    "    # NODE 1 OUTPUT\n",
    "    node_1_output = input1 * w1 + input2 * w3 + bias1\n",
    "    node_1_output = activation_ReLu(node_1_output)\n",
    "\n",
    "    # NODE 2 OUTPUT\n",
    "    node_2_output = input1 * w2 + input2 * w4 + bias2\n",
    "    node_2_output = activation_ReLu(node_2_output)\n",
    "\n",
    "    # NODE 3 OUTPUT\n",
    "    # we can just use Node 1 and 2 outputs, since they\n",
    "    # already contain the previous weights in their result\n",
    "    node_3_output = node_1_output * w5 + node_2_output * w6 + bias3\n",
    "    node_3_output = activation_ReLu(node_3_output)\n",
    "    return node_3_output\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 6, 9]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.597078202684607"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try using the model with our prediction function\n",
    "# the value tends to be same as final bias 3\n",
    "# so if node1 and node2 outputs are small => more or less bias3\n",
    "result = predict(1, 6)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
